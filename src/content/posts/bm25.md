---
title: "Ranking 101: Why BM25 Still Wins More Than It Should"
pubDate: 2025-08-11
modDate: 2025-08-11
categories: ["information retrieval", "ranking", "search", "experiments"]
description: "The decades-old algorithm that still embarrasses neural models if you're not careful — and how to use it properly."
slug: bm25-baseline
draft: false
pin: false
---

# Ranking 101: Why BM25 Still Wins More Than It Should

If you care about building good search or retrieval systems — whether it’s for documents, products, code, or even emoji — you’ll eventually come across **BM25**.

And if you’re not careful, you’ll dismiss it as old tech.

Which is funny, because BM25 still acts like it has something to prove. In nearly every ranking competition or paper I’ve touched, you’ll see something like:

> “Our neural model beats BM25 by 4.2 points (but only after fine-tuning for 3 days).”

This post is a reminder of just how good a properly tuned BM25 baseline can be — and why it should be the *first thing you try*, not the last.

---

### What is BM25?

It’s a classic bag-of-words ranking function that scores documents against a query using:

* **Term frequency** (how often does the term appear?)
* **Inverse document frequency** (how rare is the term?)
* **Document length normalization** (shorter docs aren’t unfairly favored)

The scoring function:

$$
\text{score}(q, d) = \sum_{i=1}^{|q|} \text{IDF}(q_i) \cdot \frac{f(q_i, d) \cdot (k_1 + 1)}{f(q_i, d) + k_1 \cdot (1 - b + b \cdot \frac{|d|}{\text{avgdl}})}
$$

Where:

* $f(q_i, d)$: frequency of query term $q_i$ in document $d$
* $|d|$: length of the document
* $\text{avgdl}$: average doc length across the corpus
* $k_1$ and $b$: hyperparameters

yeap it looks scary 
---

### My experiment setup

I pulled a mini version of the [MS MARCO](https://microsoft.github.io/msmarco/) dataset — just 10k query-passage pairs — and benchmarked:

* BM25 using `pyserini`
* Naive TF-IDF
* Random shuffle (for baseline)

```python
from pyserini.search.lucene import LuceneSearcher

searcher = LuceneSearcher.from_prebuilt_index('msmarco-passage')
hits = searcher.search("what is ewma volatility", k=10)

for i in range(10):
    print(f'{i+1:2} {hits[i].docid} {hits[i].score:.5f}')
```

Pyserini handles the heavy lifting — tokenization, indexing, retrieval — and is backed by Lucene under the hood.

---

### What surprised me

* **Tuning matters**. BM25 has only two hyperparameters (`k1`, `b`) but they *hugely* affect ranking quality.
* **BM25 still wins**. On short queries and medium-sized corpora, even a fine-tuned dense retriever struggles to consistently outperform it.
* **Neural models cheat**. They often rely on seeing relevance judgments during training — BM25 doesn’t.

---

### What breaks it

* Synonyms: “president” vs. “head of state” — BM25 doesn’t generalize.
* Misspellings, typos, and morphology — neural models thrive here.
* Cross-lingual, low-resource, or nuance-heavy queries — BM25 just counts words.

---

### Subtle résumé thread

In my ranking stack experiments (project: Neuro-Ranker), BM25 is the first stage of candidate generation before a neural cross-encoder re-ranks. Without a solid BM25 base, the re-ranker never sees the best options. Performance isn't just about model quality — it's about **good inputs to good models**.

---

### What I’d try next

* Switch to **SPLADE** (sparse neural) and compare retrieval/latency tradeoffs
* Train a **bi-encoder** using the BM25 top-k as positives
* Evaluate under **recall\@10**, **NDCG**, and **mean reciprocal rank (MRR)** properly

---

### TL;DR

If you care about search quality and reproducibility, BM25 should be your best friend.
It’s simple, robust, blazingly fast — and still good enough to embarrass fancy models when they’re misused.
